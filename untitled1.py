# -*- coding: utf-8 -*-
"""Untitled1.ipynb - Corrected Version

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tuDT1hWx-lY7gs2OODOfzZD9UAZqfbSJ
"""

# Install required packages
# pip install -q torch torchvision torchaudio
# pip install -q onnx onnxruntime
# pip install -q matplotlib seaborn scikit-learn pandas numpy tqdm

print(" Packages installed successfully!")

# 1. Check GPU availability
import torch

# Device selection
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# GPU details
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(
        f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
    )
else:
    print("‚ö† No GPU detected. Training will be slower. Consider Runtime ‚Üí Change runtime type ‚Üí GPU")

# ==============================
# Load CSV file from /content
# ==============================

import os
import pandas as pd

# CSV file path - Adjusted for local use if necessary, but keeping user provided path
csv_filename = "/content/all_segment_paired_car_following_trajectory(position-based, speed-based, processed).csv"

# Check if file exists
if os.path.exists(csv_filename):
    print(" File found")
    print(f" File name: {os.path.basename(csv_filename)}")
    print(f" File size: {os.path.getsize(csv_filename) / (1024**2):.1f} MB")

    # Load CSV
    df = pd.read_csv(csv_filename)
    print("CSV loaded successfully")
    print(" First 5 rows:")
    # display(df.head()) # display is colab/ipython specific

else:
    print(f"‚ùå File not found at {csv_filename}. Please check the path.")

# Import libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


# Set style

sns.set_style("whitegrid")
plt.rcParams["figure.figsize"] = (12, 6)

# Load data

if os.path.exists(csv_filename):
    print(" Loading dataset...")
    df = pd.read_csv(csv_filename)
    print("Dataset loaded successfully!")
    print(f" Shape: {df.shape}")
    print(f" Samples: {df.shape[0]}")
    print(f" Features: {df.shape[1]}")

    # Display columns
    print("\nüìå Columns:")
    for i, col in enumerate(df.columns, 1):
        print(f"  {i:02d}. {col}")


    # Basic info
    print("\n‚Ñπ Dataset Info:")
    df.info()


    # Check missing values
    print("\n‚ùì Missing values per column:")
    print(df.isnull().sum())

    # Dataset statistics
    print("Dataset Statistics:\n")
    print(df.describe())

    # Create unique CF pair identifier
    df['cf_pair'] = (
        df['segment_id'].astype(str) + "_" +
        df['follower_id'].astype(str) + "_" +
        df['leader_id'].astype(str)
    )


    # Sequence analysis
    seq_lengths = df.groupby('cf_pair').size()

    print("üìä Sequence Analysis:")
    print(f"üîπ Unique CF pairs: {df['cf_pair'].nunique()}")
    print(f"üîπ Min sequence length: {seq_lengths.min()}")
    print(f"üîπ Max sequence length: {seq_lengths.max()}")
    print(f"üîπ Mean sequence length: {seq_lengths.mean():.1f}")
    print(f"üîπ Median sequence length: {seq_lengths.median():.1f}")


    # Plot sequence length distribution
    plt.figure(figsize=(10, 5))
    plt.hist(seq_lengths, bins=50, edgecolor="black", alpha=0.7)
    plt.xlabel("Sequence Length (timesteps)")
    plt.ylabel("Frequency")
    plt.title("Distribution of Car-Following Sequence Lengths")
    plt.axvline(seq_lengths.mean(), linestyle="--", label=f"Mean: {seq_lengths.mean():.1f}")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

from sklearn.preprocessing import StandardScaler
import pickle

# Define features and targets
feature_cols = [
    'processed_position',
    'processed_speed',
    'processed_accer',
    'processed_jerk',
    'length',
    'local_time'
]

target_cols = [
    'processed_position',
    'processed_speed',
    'processed_accer',
    'processed_jerk'
]

if os.path.exists(csv_filename):
    print("üîπ Feature Engineering:")
    print(f"Input features: {len(feature_cols)}")
    print(f"Output targets: {len(target_cols)}")

    # Split features and targets
    X = df[feature_cols]
    y = df[target_cols]

    # Feature Scaling
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print("Features scaled successfully")

    # Save the scaler
    with open("feature_scaler.pkl", "wb") as f:
        pickle.dump(scaler, f)
    print("Scaler saved as feature_scaler.pkl")

    # Hande outliers
    print("\n Outlier Handling")
    for col in feature_cols + target_cols:
        if col in df.columns:
            lower = df[col].quantile(0.001)
            upper = df[col].quantile(0.999)
            clipped_count = ((df[col] < lower) | (df[col] > upper)).sum()
            df[col] = df[col].clip(lower=lower, upper=upper)
            if clipped_count > 0:
                print(f"‚Ä¢ {col}: Clipped {clipped_count} outliers")

    # Sort
    df_sorted = df.sort_values(['cf_pair', 'local_time']).reset_index(drop=True)
    print("Data sorted by CF pair and time")

    # Train / Val / Test Split
    unique_pairs = df['cf_pair'].unique()
    np.random.seed(42)
    np.random.shuffle(unique_pairs)
    train_idx = int(0.70 * len(unique_pairs))
    val_idx = int(0.85 * len(unique_pairs))
    train_pairs = unique_pairs[:train_idx]
    val_pairs = unique_pairs[train_idx:val_idx]
    test_pairs = unique_pairs[val_idx:]

    train_df = df[df['cf_pair'].isin(train_pairs)].copy()
    val_df   = df[df['cf_pair'].isin(val_pairs)].copy()
    test_df  = df[df['cf_pair'].isin(test_pairs)].copy()

# Functions for sequence windowing
from typing import Tuple
def create_sequences(df, feature_cols, target_cols, seq_length=10, pred_horizon=10):
    X_list, y_list = [], []
    for cf_pair in df['cf_pair'].unique():
        pair_data = df[df['cf_pair'] == cf_pair].reset_index(drop=True)
        if len(pair_data) < seq_length + pred_horizon: continue
        for i in range(len(pair_data) - seq_length - pred_horizon + 1):
            X_list.append(pair_data.iloc[i:i+seq_length][feature_cols].values)
            y_list.append(pair_data.iloc[i+seq_length:i+seq_length+pred_horizon][target_cols].values)
    return np.array(X_list), np.array(y_list)

# Define LSTM Model
import torch.nn as nn
class LSTMCarFollowingModel(nn.Module):
    def __init__(self, n_features, n_targets, hidden_size=128, num_layers=2, dropout=0.3, pred_horizon=10, bidirectional=True):
        super(LSTMCarFollowingModel, self).__init__()
        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0, bidirectional=bidirectional)
        lstm_out_size = hidden_size * 2 if bidirectional else hidden_size
        self.fc1 = nn.Linear(lstm_out_size, 64)
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(64, pred_horizon * n_targets)
        self.relu = nn.ReLU()
        self.pred_horizon = pred_horizon
        self.n_targets = n_targets
        self.bidirectional = bidirectional

    def forward(self, x):
        lstm_out, (h_n, c_n) = self.lstm(x)
        if self.bidirectional:
            h_n = torch.cat((h_n[-2], h_n[-1]), dim=1)
        else:
            h_n = h_n[-1]
        out = self.relu(self.fc1(h_n))
        out = self.dropout(out)
        out = self.fc2(out)
        return out.view(-1, self.pred_horizon, self.n_targets)

print("Architecture defined.")
